> 前言： 本次笔记对《强化学习（第二版）》第十一章进行概括性描述。

以下概括都是基于我个人的理解，可能有误，欢迎交流：piperliu@qq.com。

总的来说，第11章学习体验不好。可能是由于内容本身比较抽象，第11章属于星标章节。练习题没有引起我的兴趣。还有一点比较令人失望：尽管本章讨论了不少更新目标与算法（其中很多为反例），并给出了大量带有矩阵的计算公式，但实例并不多。因此，我认为理解其大概思想便可。

# 正文

`基于函数逼近的离轨策略方法`的大多方法并不令人满意：无论是理论上还是实践上。

离轨策略学习目前面临两个挑战：
- The first part of the challenge has to do with `the target of the update` (not to be confused with the
target policy);
- and the second part has to do with the `distribution of the updates`.

解释一下，在离轨策略的预测/控制中，我们更新的目标与交互的基础是不同的，因此有了第一个挑战。在非表格型情况下，我们需要用到`状态的分布`$\mu(s)$，因此有了第二个挑战。

第一个挑战可以通过引入`重要度采样`来解决。

### 11.1 半梯度方法

本节讨论了`重要度采样`在离轨策略中的推广。**换句话说，就是把之前表格型算法中的离轨方法转换为半梯度形式。**

值得注意的是，这些方法只对`表格型`的情况才保证稳定且渐进无偏。

### 11.2 离轨策略发散的例子

本节举了一个小例子和一个大例子，说明了半梯度以及其他方法的`不稳定性`。

Baird 反例代码见：[https://nbviewer.jupyter.org/github/PiperLiu/Reinforcement-Learning-practice-zh/blob/master/practice/Counterexample.ipynb](https://nbviewer.jupyter.org/github/PiperLiu/Reinforcement-Learning-practice-zh/blob/master/practice/Counterexample.ipynb)

此外，Tsitsiklis 和 van Roy's 的反例由数学推导得证。

这里我要强调一下 Baird 反例，这着实困扰我一些时间。并且现在也没有完全明白其计算过程。我能给出的说明是：`如果你对于为什么状态的编码刑如 2w_1 + w_8 有疑问，那么我的解释是，状态编码（或者说线性特征的构造）并不是我们决定的，在例子中，特征已经被很愚蠢地构造好了，在这种愚蠢地构造下，我们可以观测大，传统的半梯度方法会导致不收敛、不稳定。`

那么，既然如此，我们“聪明地”构造特征不就可以了吗？

不可以，因为大部分情况下，我们无法掌控我们对于特征的构造（因为`权值向量`$w$维度不能太大）。如果要构造特征的话，这涉及到$w$到$s$的映射关系（我的理解是，`神经网络`/`瓦片编码`可以解决这种问题）。这也是为什么在本节末尾，提到了“`另一个避免不稳定的方式是使用函数逼近中的一些特殊方法`”。

### 11.3 致命三要素

`同时满足`以下三种要素，一定不稳定/发散：
- 函数逼近
- 自举法
- 离轨策略训练

那么，该抛弃哪个来保证稳定呢？

一、函数逼近极大地节省了计算开销，不能抛弃。

二、自举法或许可以抛弃，但是将带来的效率损失是巨大的：
- 不用自举法学的慢，在第10章中已经有实例证明；
- 使用蒙特卡洛将带来巨大的存储压力。

三、离轨策略或许可能被抛弃。因为很多情况下，同轨策略就住够了。但是想实现真正的“通用性”，仅仅依靠离轨策略是不够的。

### 11.4 线性价值函数的几何性质

这节课为向量赋予了“几何性质”，但着实困扰我许久。这节课是`后面章节所提指标的铺垫`。

书上说，考虑这种情况：
- $S = \{ s_1,s_2,s_3 \}$
- $w = (w_1, w_2)^T$

接着，书上就做了二维与三维交叉的图，`这让我很困惑`。最终我给自己的解释是（`这个解释存疑`）：这个空间中的点对应的三维坐标是$w$对应的各状态的价值，$v(s)$是三元组$(v(s_1),v(s_2),v(s_3))$。

近似的`线性`价值函数即：

$$v_w = X w$$

其中：

$$X = |S| \times d = \left[ \begin{aligned}
x_1(s) \; x_2(s) \\
x_1(s) \; x_2(s) \\
x_1(s) \; x_2(s) \\
\end{aligned} \right]$$

这里注意理解一下，$S$本来是有`三`个维度的，因为`“特征工程”`的处理，变成了只有两个特征的$X$。为什么两个特征？因为我们的参数只有两个值（$w=(w_1,w_2)^T$）。

如果仅仅依靠线性关系来计算，因为我们的参数只有两个值（$w=(w_1,w_2)^T$），并且$X$可能很差，因此（考虑`向量相乘的几何意义`尤其是`方向`）我们得到的状态价值$(\hat{v}(s_1),\hat{v}(s_2),\hat{v}(s_3))$这个向量很有可能无法得到$v_\pi$。

值得注意的是，这里我们描绘距离不使用欧几里得距离。

### 11.5 对贝尔曼误差做梯度下降

本节中以最小化`均方梯度误差`$\overline{TDE}$为目标，使用完全的 SGD 算法进行下降。这在理论上会保证收敛，这个算法被称为`朴素残差梯度` 算法。但是，**朴素残差收敛到的并非最优策略。**

书上例11.2的`A-分裂`例子很好地做了反例。

因此，我们考虑贝尔曼误差$\overline{BE}$。如果价值是准确的，那么贝尔曼误差是0。

贝尔曼误差是该状态的TD误差的期望。需要下一状态的`两个独立样本`，但这并不难实现。且，贝尔曼误差在现行情况下，总会收敛到最小化对应的$w$，且$w$是唯一的。

以贝尔曼误差为目标下降

但问题有三个：
- 贝尔曼误差对应的`残差梯度算法`太慢了；
- 贝尔曼误差看起来仍然收敛到错误的值（书例11.3 A预先分裂的例子）；
- 贝尔曼误差是不可学习的。

### 11.6 贝尔曼误差是不可学习的

何为不可学习的的？

即，不能从可观测的数据中学到。

书上有例子：两个情况是不同的，但产生的数据遵从同一分布，而$\overline{VE}$却不同。也就是说$\overline{VE}$并`不是这个数据分布唯一确定的函数`（跟数据序列竟然没什么关系！）。$\overline{BE}$也是一个道理。因此$\overline{BE}$是不可学习的。

但是，为啥在上一章我们认为$\overline{VE}$可用呢？因为优化他的参数是可学习的。我们引出`均方回报误差`$\overline{RE}$。

### 11.7 梯度TD方法

考虑最小化`均方投影贝尔曼误差`$\overline{PBE}$的SGD。

本节由数学推导，依此提出了`最小均方（Least Mean Square, LMS）`、`GTD2`、`带梯度修正的TD(0)(TDC)或者GTD(0)`算法。

实例证明，梯度TD(0)是有用的。

GTD2 和 TDC 其实包含了两个学习过程：学w和学v。书上将这种不对称的依赖称为`梯级`。

### 11.8 强调 TD 方法

此外，书上还简单介绍了强调 TD 方法，核心思想是将更新分布也转换为同轨策略分布。

这需要用到`兴趣值`与`强调值`。

理论上，期望中，算法理论上是可以收敛到最优解的，但实际上并非如此。

### 11.9 减小方差

介绍了“重要性权重感知”更新、树回溯算法、“识别器”等概念，或许可以帮助减小估计的方差，更有效地利用数据。

PiperLiu
2020-3-15 16:39:22