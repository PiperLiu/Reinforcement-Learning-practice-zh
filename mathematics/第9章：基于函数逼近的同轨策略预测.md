> **前言：** 本次笔记对《强化学习（第二版）》第九章进行概括性描述。

*以下概括都是基于我个人的理解，可能有误，欢迎交流：piperliu@qq.com。*

# 正文

### 引言

前八章学习的“表格型方法”有一个问题：没法表示任意大的状态空间，比如连续状态。

并且，在连续状态中，我们相信描述状态的数值与价值间存在数量关系，这意味着我们可以用$v(s,w)$来描述$v$，而并非要建立$s \rightarrow v$的表格。

其中，$w$是向量（叫做`权值向量`），我们从已知状态归纳所求状态的行为属于`泛化`，通常使用`函数逼近`（`监督学习`范畴）。

### 价值函数逼近

显然，我们想要“基于函数逼近的同轨策略预测”，就是要更新$w$。

但是，强化学习与监督学习不同的一点是，强化学习强调**在线学习&与环境交互**。

这意味着我们还需要算法可以处理非平稳状况。

### 预测目标($\overline{VE}$)

表格型学习中，在每个状态下学习价值函数是`解耦`的：即一个状态更新不会影响其他状态。但是在函数逼近中未必。

我们使用`均方价值误差`作为目标函数。强化学习的终极目标是寻找更好的策略，因此使用`均方价值误差`作为目标函数未必是最好的，但是目前看来有用。

$$\overline{VE} (w) = \sum_{s \in S} \mu(s) [v_\pi (s) - \hat{v} (s,w)]^2$$

其中$\mu(s)$表示分布。这很 make sense 。

### 随机梯度和半梯度方法

`梯度下降`很基础很直观，这里不做介绍。

随机梯度下降英文：stochastic gradient-descent, SGD。之所以叫“随机”，因为其更新仅仅依赖一个“随机”的样本完成。

其更新公式为：

$$w \leftarrow w + \alpha [G_t - \hat{v}(S_t, w) ]\nabla \hat{v} (S_t,w)$$

这个公式很核心。书9.3节中“梯度蒙特卡洛”伪代码中使用了上式。依此估计的价值是无偏的。

**但是，“半梯度”是有偏的。**

在9.3节“半梯度TD(0)”中使用了如下的更新公式：

$$w \leftarrow w + \alpha [R + \gamma \hat{v}(S', w) - \hat{v}(S_t, w) ]\nabla \hat{v} (S_t,w)$$

为什么叫半梯度：
- 非半梯度中，更新依赖于$\hat{v}(S', w)$，其中含有$w$，并不只是真正的梯度下降；
- 估计是有偏的。

但是，半梯度的优点：
- 速度快；
- 可以在线并持续地学习，无需等待幕结束；
- 线性情况中，依旧可以稳健的收敛。**而线性情况及其常见。**

### 线性方法

线性方法中，$\hat{v}(s,w) = w^T x(s)=\sum_{i=1}^d w_i x_i (s)$，向量$x (s)$称为特征向量。

在线性TD(0)中，我们可以证明其预测时收敛的，并且将收敛到`TD不动点`：

$$w_{TD} = A^{-1} b$$

$$A = \mathbb{E} [x_t (x_t - \gamma x_{t+1})^T] \in \mathbb{R}^d \times \mathbb{R}^d$$

$$b=\mathbb{E} [R_{t+1}x_t] \in \mathbb{R}^b$$


这对于后面的“最小二乘”很重要。

**更有趣的是：第I部分表格型方法实际上是线性函数逼近的一个特例，在这种情况下，$\nabla \hat{v} (S_t,w)=1$，即特征向量是$x(s) = 1$。**

更进一步，表格型方法中$w(S_t)$就是$\hat{v} (S_t,w)$本身。

### 线性方法的特征构造

这里有很多有趣的操作，比如`多项式基`、`傅立叶基`、`粗编码`、`瓦片编码`（一种粗编码的实例）等。

在我看来，前两者就是“特征工程”，使用线性计算的方法考虑状态间的非线性（交互）特性。

后两者类似一种聚合，但是建立了多个聚合的“标准”，使得一个状态在不同的瓦片中有可能不同的“号码”。

在瓦片的实现中，可以使用哈希编码节省时空。但是书上没有具体实例。

此外，还有`径向基函数`。

其对特征的处理类似高斯分布，使用了`范数`/`距离度量`的概念，但是范数未必是距离的度量，可以设定别的规则。

### 手动选择步长参数

书上建议将 线性SGD 步长设置为：$\alpha = (\tau \mathbb{E}[x^T x])^{-1}$

### 非线性函数逼近：人工神经网络

人工神经网络在DL中很常用，书上仅做了描述性介绍。

这里提几个名词：
- 深度置信网络；
- 批量块归一化；
- 深度残差学习。

### 最小二乘时序差分

之前提到了，使用不动点的计算在数学上很有效（数学上证明是合理的）。且，其对数据的利用是充分的。

最小二乘时序差分对 A、b公式进行了变换，令A多了一个项$\epsilon I$以保证其永远可逆。

这样，就可以将一个统计性公式转换成可在线更新的迭代式了。

### 基于记忆的函数逼近

在这里，讨论了`拖延学习 lazy learning`，即不立即更新并使用参数；当我们需要预测某个状态时，我们根据已有的数据（记忆中的状态），找出与其相关的，并计算估计。

常用的例子有：`最近邻居法`，`加权平均法`等。

这可以有效地进行局部学习，但是查询速度称为一个问题。因此可以考虑`特制硬件`、`并行计算`、`数据结构`等方案以解决查询速度问题。

### 基于核函数的函数逼近

之前`径向基函数`中有提到距离这个概念。`核函数`则是决定`距离`这个函数如何映射的。

$D$是一组样本，$g(s')$表示状态$s'$的目标结果，$\hat{v}(s,D) = \sum_{s' \in D}k(s,s')g(s')$这个公式可以看出，核函数决定了权重（或者是否计算，因为核函数为了保证局部计算，将不相干状态间映射为0）。

### 深入了解同轨策略学习：“兴趣”与“强调”

思想是不变的，还是为了保证学习的精准性，即利用到“局部学习”这个概念。

也就是说，不应把所有状态平等地看待，因此在更新公式中增加了`强调值`与`兴趣值`，并且二者也进行递归。

2020-3-9 00:10:21
PiperLIu