> **前言：** Sutton第二版《强化学习》中，第4章第2节“策略改进”介绍了基于贪心算法的策略改进。为什么以（只考虑一个后续状态来选择当前动作的）贪心算法进行更新的策略一定会比原策略更优呢？书上也给出了论证，但不明显。这里我将其整理，把顺序和逻辑重新捋一下。

#### 定义：策略A比策略B好

如果$\pi$和$\pi '$是两个确定的策略，对任意$s \in S$，有：

$$q_\pi (s, \pi ' (s)) \ge v_\pi (s)$$

我们称$\pi '$相比于$\pi$更好。*$q$为期望，$v$为价值。*

#### 策略A比B好，意味着A更高的价值

将式子$q_\pi (s, \pi ' (s)) \ge v_\pi (s)$展开：

$$
\begin{aligned}
v_\pi (s) & \le q_\pi (s, \pi ' (s)) \\
& ... \\
& \le \mathbb{E}_{\pi '} [R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... | S_t = s] \\
&  = v_{\pi '} (s)
\end{aligned}
$$

由此式，可以推导出，若对于状态$s$，选择期望更高的动作（选择更好的策略），**可以使其价值提高。**

**tips:** 上式的推导步骤省略了，其推导的核心就是应用$q$的定义式，迭代展开$q$：

$$
\begin{aligned}
q_{\pi} (s, \pi' (s)) & = \mathbb{E} [R_{t+1} + \gamma v_\pi (S_{t+1} )| S_t = s, A_t = \pi' (s)]\\
& = \mathbb{E}_{\pi '} [R_{t+1} + \gamma v_\pi (S_{t+1} )| S_t = s] \\
& \le \mathbb{E}_{\pi '} [R_{t+1} + \gamma q_{\pi} (S_{t+1}, \pi' (S_{t+1}))| S_t = s] \\
& ... \\
\end{aligned}
$$

#### 策略改进：贪心算法构造更好的策略

在每个状态下根据$q_\pi (s, a)$选择一个最优的，换言之，考虑一个新的贪心策略$\pi '$，满足：

$$\begin{aligned}
\pi's (s) & = \argmax_a q_\pi (s, a) \\
& = argmax_a \mathbb{E} [R_{t+1} + \gamma v_\pi (S_{t+1} )| S_t = s, A_t = \pi' (s)] \\
& = argmax_a \left\{ \sum_{s', r} p(s', r | s, a)[r + \gamma v_\pi (s')] \right\}
\end{aligned}$$

这个贪心策略采取在短期内看上去最优策略，根据$v_{\pi}$单步搜索。

#### 反证法证明：策略改进是“有效的”

我们知道，贪心策略基于$q_\pi (s, \pi ' (s)) \ge v_\pi (s)$，可以保证$v_{\pi '} (s) \ge v_\pi (s)$。

但是，贪心策略有没有可能造成在每步搜索中（每次策略更新中），存在$v_{\pi '} (s) = v_\pi (s)$的情况（则此次更新无效）。

实际上，这是不可能的。**反证法：**

如果新的策略$\pi'$比老的策略$\pi$：$v_{\pi '} (s) = v_\pi (s)$，那么由上文贪心策略递进式，可得，对任意$s \in S$：

$$\begin{aligned}
v_{\pi '}(s) & = \max_a \mathbb{E} [R_{t+1} + \gamma v_{\pi '} (S_{t+1} )| S_t = s, A_t = a] \\
& = \max_a \sum_{s', r} p(s', r | s, a)[r + \gamma v_{\pi '} (s')] 
\end{aligned}$$

这个式子就是贝尔曼最优方程组了，所求解的$v_{\pi '}$一定位最优策略下的$v_{*}$。

因此，如果如果新的策略$\pi'$比老的策略$\pi$：$v_{\pi '} (s) = v_\pi (s)$，那么一定是已经迭代到了最优的策略，否则，策略改进一定会给出一个更优的结果。

今天我们讲的内容，为以后的工程问题奠定了理论基础，有了理论依据，即：我依据贝尔曼方程更新策略是有效的。
