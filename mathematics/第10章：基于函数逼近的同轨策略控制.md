> 前言： 本次笔记对《强化学习（第二版）》第十章进行概括性描述。

以下概括都是基于我个人的理解，可能有误，欢迎交流：piperliu@qq.com。

# 正文

很短的一章，是对于第九章的延续。

第九章中，我们用`函数逼近`的方法为$V(S)$估值，而第十章中，我们用同样的思想为$q_\pi(s,a)$估值。但是具体内容上，又是大不同的。

### 10.1 - 10.2

这两节内容为：
- 10.1 分幕式半梯度控制
- 10.2 半梯度 n 步 Sarsa

这两节是第9章内容延续，没有什么好说的。在`程序实现`中，值得强调的有两点：
- 从第9章开始，我们假设了$w$与目标向量$v$或$q$间是`线性关系`，因此可以把$w$理解成`“权重”`，寻找最优动作时，找权重最大的就可以了；
- `瓦片编码`，在第9章中，我们对状态$s$瓦片编码，而第10章中，我们对$[s,q]$进行瓦片编码。如此，就与表格有所不同。`换句话说，如果不进行瓦片编码，那么将面临这个大问题：`**单纯对(s,a)进行聚合，与表格型求解无差别，(s,a)间是解耦。因此，使用瓦片编码，使(s,a)间耦合。**

以上，是我看了很久 Shantong Zhang （Sutton 学生，官方认可的代码）的代码后得出的结论。

瓦片编码采用了哈希方法，由 Sutton 本人制作 `python3` 文件：[http://incompleteideas.net/tiles/tiles3.py-remove](http://incompleteideas.net/tiles/tiles3.py-remove)

我对代码进行了一点标注，见：[https://nbviewer.jupyter.org/github/PiperLiu/Reinforcement-Learning-practice-zh/blob/master/practice/Mountain-Car-Acess-Control.ipynb](https://nbviewer.jupyter.org/github/PiperLiu/Reinforcement-Learning-practice-zh/blob/master/practice/Mountain-Car-Acess-Control.ipynb)

### 10.3 - 10.5

这三节内容为：
- 10.3 平均收益：持续性任务中的新的问题设定
- 10.4 弃用折扣
- 10.5 差分半梯度 n 步 Sarsa

为了解决持续性问题的控制问题，10.3与10.4 引出了`平均收益`、`差分回报`与`差分价值函数`的概念，并且在数学上证明了：**持续性问题中折扣的无用性。** 这个证明是基于**MDP的便利性假设的。**

与权重$w$同，平均收益虽然客观存在，但是不可知。因此平均收益也是需要更新的，这个过程也需要定义步长。

### 对练习题解的补充

练习10.6很有趣。解答参见：[https://github.com/PiperLiu/Reinforcement-Learning-practice-zh/tree/master/resources](https://github.com/PiperLiu/Reinforcement-Learning-practice-zh/tree/master/resources)中的`RL-An Introdction exercises Solution.pdf`。

> tip: 我知道了什么叫`同余方程`。

这里解释一下，为什么 $V(A) = -\overline{R} + V(B)$。

我的理解是这样的：

$$\lim_{t \to 0} \delta_t = 0$$

而

$$\delta_t = R_{t+1} - \overline{R}_{t+1} + \hat{v}(S_{t+1}, w_t) - \hat{v}(S_t,w_t)$$

结合例题中的情况则是：

$$0 = 0 - \frac{1}{3} + V(B) - V(A)$$

有关$V(C)$的式子同理。

- - -

练习10.7同样很有趣。**但是 Bryn Elesedy 提供的答案中，有处`笔误`。**

**原文如下：**

Now to compute the differential state values we write

$$   V(S; \gamma) = \lim_{h\to \infty} \sum_{t=0}^h \gamma^t \left( \mathbb{E}[R_{t+1} \vert{} S_0 = s] - \bar{R} \right)$$
then

$$
\begin{aligned}
    V(A; \gamma) &= 1 - \bar{R} + \gamma V(A; \gamma) \\ 
    V(A; \gamma) &= - \bar{R} + \gamma V(B; \gamma)
\end{aligned}
$$

so

$$
    V(A; \gamma) = \frac12 ( 1 - \gamma ) - \gamma^2 V(A; \gamma)
$$

**我对其的修改与解释如下：**

在例题的情况中，因为原差分回报公式$G_t(formula \; 10.9)$的定义不使用，因此对于状态的价值，本题中采用：

$$   V(S; \gamma) = \lim_{h\to \infty} \sum_{t=0}^h \gamma^t \left( \mathbb{E}[R_{t+1} \vert{} S_0 = S] - \bar{R} \right)$$

那么，由上式，可得：

$$
\begin{aligned}
    V(B; \gamma) &= \lim_{h\to \infty} \sum_{t=0}^h \gamma^t \left( \mathbb{E}[R_{t+1} \vert{} S_0 = B] - \bar{R} \right) \\
    & = \lim_{h\to \infty} \gamma^0 (\mathbb{E}[R_{1} \vert{} S_0 = B] - \bar{R}) + \lim_{h\to \infty} \sum_{t=1}^h \gamma^t \left( \mathbb{E}[R_{t+1} \vert{} S_0 = B] - \bar{R} \right)\\
    & = 1 - \bar{R} + \gamma V(A; \gamma) \\
    V(A; \gamma) &= - \bar{R} + \gamma V(B; \gamma)
\end{aligned}
$$

解二元一次方程组：

$$
    V(A; \gamma) = \frac12 ( 1 - \gamma ) - \gamma^2 V(A; \gamma)
$$

- - -

练习10.8很生动、巧妙地告诉我们，使用书上式子10.10对$\bar{R}$进行更新，“Once $\bar{R}$
gets to the correct value it never leaves”。
